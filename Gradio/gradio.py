import gradio as gr
import openai
from  openai import OpenAI
import base64
import os
import requests

# OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "https://dfqnnshbcpffgkps.tunnel-pt.elice.io/proxy/8007/v1"

client = OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key=openai_api_key,
    base_url=openai_api_base,
)

fast_api_base = "http://localhost:8000/"

prompt_rag = "Please provide the major issues of 2024. It would be great if you could combine the information I provide with your own knowledge."

flag = 0

def main():

    models = client.models.list()
    model = models.data[0].id
    print(f"Using model: {model}")

    def encode_base64_content_from_file(image_path: str) -> str:
        """Encode a content retrieved from image_path to base64 format."""
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
        return encoded_string
    
    # Image URL and text input inference
    def run_image_and_text_inference(image_base64, question) -> str:
        # Constructing the messages with both text and image content
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt_rag + question},
                    {"type": "image_url", 
                     "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}},
                ],
            }
        ]

        # Send the combined input to the model
        chat_completion = client.chat.completions.create(
            model=model,  # Adjust model as per availability
            messages=messages,
            max_tokens=512,  # Configurable for more detailed responses
        )

        result = chat_completion.choices[0].message.content
        return result
    
    def run_text_inference(question) -> str:
        messages = [
            {
                "role": "user",
                "content": prompt_rag + question
            }
        ]

        chat_completion = client.chat.completions.create(
            model=model, 
            messages=messages,
            max_tokens=512,
        )

        result = chat_completion.choices[0].message.content
        return result
    
    # ê³µí†µ ì¸í¼ëŸ°ìŠ¤ í•¨ìˆ˜
    def run_inference(question, chat_history=None, image_path=None):
        messages = []

        # ì´ì „ ì±„íŒ… ê¸°ë¡ì´ ìˆëŠ” ê²½ìš° ë©”ì‹œì§€ êµ¬ì„±
        if chat_history:
            for msg in chat_history:
                if flag==1:
                    print('ì¡°ê±´ê±¸ë¦¼!!!!')
                    # ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í¬í•¨í•œ ê²½ìš°, ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ messagesì— ì¶”ê°€
                    image_base64 = encode_base64_content_from_file(image_path)
                    messages.append({
                        "role": msg["role"],
                        "content": [
                            {"type": "text", "text": question},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                        ],
                    })
                else:
                    # í…ìŠ¤íŠ¸ ë©”ì‹œì§€ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶”ê°€
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })

        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if image_path:
            image_base64 = encode_base64_content_from_file(image_path)
            messages.append({
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                ],
            })
        else:
            # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
            messages.append({"role": "user", "content": question})

        # ëª¨ë¸ì—ê²Œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì„œ ì‘ë‹µ ë°›ê¸°
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=512,
        )

        answer = response.choices[0].message.content
        return answer

    def retrieval(question, image):
        if image:
            endpoint = "retrieval/file"
            url = f"{fast_api_base}{endpoint}"

            with open(image, "rb") as f:
                files = {"file": f}
                response_retrieve = requests.post(url, params={"query": question}, files=files)
            
            image_base64 = encode_base64_content_from_file(image)
            
            if response_retrieve.status_code == 200:
                json_data = response_retrieve.json()
                retrieved_chunks = json_data.get("retrieved_chunks", [])
                combined_text = " ".join(retrieved_chunks)
                response = run_image_and_text_inference(image_base64, combined_text + question)
                return response
            else:
                response = run_image_and_text_inference(image_base64, question)
                return response
        else:
            endpoint = "retrieval"
            url = f"{fast_api_base}{endpoint}"

            response_retrieve = requests.get(url, params={"query": question})

            if response_retrieve.status_code == 200:
                json_data = response_retrieve.json()
                retrieved_chunks = json_data.get("retrieved_chunks", [])
                combined_text = " ".join(retrieved_chunks)
                response = run_text_inference(combined_text + question)
                return response
            else:
                response = run_text_inference(question)
                return response
        
    def init():
        endpoint = "init/index"
        url = f"{fast_api_base}{endpoint}"
        response = requests.post(url)
        
        if response.status_code == 200:
            return response.json().get("message", "Indexing Complete")
        else:
            return f"Indexing Failed: {response.status_code}, {response.text}"
        
    
    def generate_index(uploaded_file):
        endpoint = "indexing/file"
        url = f"{fast_api_base}{endpoint}"

        if not os.path.exists(uploaded_file.name):
            return "Indexing Failed: File not found"
        
        file_extension = os.path.splitext(uploaded_file.name)[-1].lower()
        if file_extension not in [".pdf", ".jpg", ".jpeg", ".png"]:
            return f"Indexing Failed: Unsupported file type {file_extension}"

        with open(uploaded_file.name, "rb") as f:
            files = {"file": f}
            response = requests.post(url, files=files)

        if response.status_code == 200:
            return response.json().get("message", "Init Complete")
        else:
            return f"Indexing Failed: {response.status_code}, {response.text}"
        
    def respond(message, chat_history):
        global flag
        # # ì‚¬ìš©ìê°€ ì•„ë¬´ëŸ° ì…ë ¥ì„ í•˜ì§€ ì•Šì€ ê²½ìš° í”„ë¡¬í”„íŠ¸ ì„ íƒ
        # if not message:
        #     message = random.choice(prompts)

        # ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸ (ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì§€ ì•Šì€ì§€ í™•ì¸)
        image_path = None
        if isinstance(message, dict) and isinstance(message.get("files"), list) and len(message.get("files")) > 0:
            image_path = message.get("files")[0]


        # í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶”ì¶œ
        question = message.get("text") if isinstance(message, dict) else message

        # ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ì…ë ¥í–ˆì„ ê²½ìš°, ì¶”ê°€ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€ (í™”ë©´ì—ëŠ” ë‚˜íƒ€ë‚˜ì§€ ì•Šê²Œ ì²˜ë¦¬)
        combined_question = question
        #if question:
        #   additional_prompt = random.choice(prompts)
            # ì§ˆë¬¸ê³¼ ì¶”ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•© (ì‹¤ì œ ëª¨ë¸ì— ì „ë‹¬í•  ë•Œë§Œ ì‚¬ìš©)
        #  combined_question += f" {additional_prompt}"

        # ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰ (ê²°í•©ëœ ì§ˆë¬¸ìœ¼ë¡œ ì‹¤í–‰)
        bot_message = run_inference(combined_question, chat_history, image_path)

        # ì±„íŒ… ê¸°ë¡ ì—…ë°ì´íŠ¸ (í™”ë©´ì—ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë˜ ì§ˆë¬¸ë§Œ ë‚˜íƒ€ë‚˜ê²Œ í•¨)
        if question:
            chat_history.append({"role": "user", "content": question})

        if image_path:
            flag = 1
            chat_history.append({"role": "user", "content": {"path": image_path}})

        # ëª¨ë¸ì˜ ì‘ë‹µì„ ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
        chat_history.append({"role": "assistant", "content": bot_message})

        print('íˆìŠ¤í† ë¦¬ ë‚´ì—­', chat_history)
        
        return "", chat_history

    def clear_chat():
        return "", []


    with gr.Blocks(css=""" 
    #chatbot .message {
        border-radius: 12px;
        padding: 10px 15px;
        margin-bottom: 10px;
    }
    #chatbot .message.user {
        background-color: gray;
        color: #0d6efd;
        text-align: left;
    }
    #chatbot .message.assistant {
        background-color: #f8f9fa;
        color: #495057;
        text-align: left;
    }
    .multimodal-textbox {
        border: 2px solid #0d6efd;
        border-radius: 8px;
        padding: 10px;
        margin: 10px 0;
    }
    .btn-primary {
        background-color: #0d6efd;
        color: white;
        border: none;
        border-radius: 8px;
        padding: 10px 20px;
        margin-top: 10px;
    }
    .btn-primary:hover {
        background-color: #084298;
        color: white;
    }
    body {
        font-family: Arial, sans-serif;
        background-color: white;
    }
""") as demo:
        gr.Markdown(
            """
            # ğŸ†Information Chatbot
            Ask questions
            """,
            elem_id="header",
        )
        chatbot = gr.Chatbot(type="messages")
        msg = gr.MultimodalTextbox(
            interactive=True,
            file_count="multiple",
            placeholder="Enter message or upload file...",
            show_label=False,
        )
        clear_button = gr.Button("ì‚¬ì§„ì€ í•œ ë²ˆì— í•˜ë‚˜ë§Œ ì˜¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ì§„ì„ ì¶”ê°€ë¡œ ì—…ë¡œë“œí•˜ê³  ì‹¶ë‹¤ë©´ Clearí•˜ì„¸ìš”!", elem_classes="btn-primary")

        clear_button.click(fn=clear_chat, inputs=[], outputs=[msg, chatbot])
        msg.submit(respond, [msg, chatbot], [msg, chatbot])


        gr.Markdown("# RAG-Indexing")
        
        # file Indexing
        file_input = gr.File(label="Upload PDF or Image Files", file_types=[".pdf", ".jpg", ".jpeg", ".png"])

        with gr.Row():
            indexing_btn = gr.Button("Indexing")
            init_btn = gr.Button("Init")     
        indexing_output = gr.Textbox(label="Indexing Status", placeholder="Please provide reliable information!")
        indexing_btn.click(generate_index, inputs=[file_input], outputs=[indexing_output])
        init_btn.click(init, None, outputs=[indexing_output])

        gr.Markdown("# RAG-Retrival")
        with gr.Row():
            image_input = gr.Image(label="Upload an Image (Optional)", type="filepath")
            text_input = gr.Textbox(label="Ask a Question", placeholder="Enter your question!")
        output_box = gr.Textbox(label="Response", placeholder="Generated response from the model")

        # Trigger response generation when both inputs are provided
        retrieval_btn = gr.Button("Retrieval")
        retrieval_btn.click(retrieval, inputs=[text_input, image_input], outputs=[output_box])

    demo.queue().launch(share=False)

if __name__ == "__main__":
    main()
